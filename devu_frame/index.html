<html>
<head>
    <title>Adjustable Virtual Glasses Try-On (Debug Log)</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.4.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.1/dist/face-landmarks-detection.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.123.0/build/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.123.0/examples/js/loaders/GLTFLoader.js"></script>
    <style>
        body {
            font-family: sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 0;
            padding: 10px;
            background-color: #f0f0f0;
        }
        .canvas-container {
            position: relative;
            width: 640px; /* Default width, will be resized */
            height: 480px; /* Default height, will be resized */
            margin-bottom: 20px;
            background-color: #333; /* Background while loading */
        }
        .canvas-container canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        #webcam {
            /* Kept hidden, used only as source */
            visibility: hidden;
            width: auto;
            height: auto;
            position: absolute;
        }
        #status {
            margin-top: 10px;
            font-size: 1.2em;
            color: #333;
        }
        .controls {
            display: flex;
            flex-direction: column;
            align-items: stretch; /* Align items to stretch */
            gap: 12px;
            margin-top: 10px;
            border: 1px solid #ccc;
            padding: 15px;
            border-radius: 8px;
            background-color: #ffffff;
            min-width: 300px; /* Give controls some width */
        }
        .controls label {
            display: flex;
            align-items: center;
            justify-content: space-between; /* Space out label and input */
            gap: 10px;
        }
        .controls span.label-text {
            white-space: nowrap; /* Prevent label text wrapping */
        }
        .controls input[type="range"] {
            flex-grow: 1; /* Allow slider to take available space */
            cursor: pointer;
        }
        .controls span.value-display {
            min-width: 35px; /* Ensure space for value */
            text-align: right;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="canvas-container" id="canvas-parent">
        <canvas id="output"></canvas> <canvas id="overlay"></canvas> </div>

    <video id="webcam" playsinline></video>

    <h1 id="status">Loading...</h1>

    <div class="controls">
        <label>
            <span class="label-text">Width Scale:</span>
            <input type="range" id="widthScale" min="0.5" max="2.0" step="0.05" value="1.0">
            <span class="value-display" id="widthValue">1.00</span>
        </label>
        <label>
            <span class="label-text">Height Scale:</span>
            <input type="range" id="heightScale" min="0.5" max="2.0" step="0.05" value="1.0">
            <span class="value-display" id="heightValue">1.00</span>
        </label>
         <label>
             <span class="label-text">V. Position:</span>
            <input type="range" id="yOffset" min="-25" max="25" step="1" value="0">
            <span class="value-display" id="yOffsetValue">0</span>
        </label>
    </div>

    <script>
        // --- DOM Elements ---
        const statusElement = document.getElementById("status");
        const webcamElement = document.getElementById("webcam");
        const outputCanvas = document.getElementById("output");
        const overlayCanvas = document.getElementById("overlay");
        const canvasParent = document.getElementById("canvas-parent");
        const widthScaleSlider = document.getElementById("widthScale");
        const heightScaleSlider = document.getElementById("heightScale");
        const yOffsetSlider = document.getElementById("yOffset");
        const widthValueSpan = document.getElementById("widthValue");
        const heightValueSpan = document.getElementById("heightValue");
        const yOffsetValueSpan = document.getElementById("yOffsetValue");

        // --- Global State ---
        let outputCtx = null;
        let model = null;
        let renderer = null;
        let scene = null;
        let camera = null;
        let glasses = null;
        let widthScaleFactor = 1.0;
        let heightScaleFactor = 1.0;
        let yOffsetFactor = 0;
        let rafId = null; // Request Animation Frame ID

        // --- Utility Functions ---
        function setText(text) {
            if (statusElement) statusElement.innerText = text;
            console.log(`STATUS: ${text}`); // Log status updates
        }

        function drawLine(ctx, x1, y1, x2, y2) {
            if (!ctx) return;
            ctx.beginPath();
            ctx.moveTo(x1, y1);
            ctx.lineTo(x2, y2);
            ctx.stroke();
        }

        // --- Core Functions ---
        async function setupWebcam() {
            console.log("Attempting to setup webcam...");
            setText("Requesting Webcam Access...");
            return new Promise((resolve, reject) => {
                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                    const errorMsg = "getUserMedia not supported by this browser.";
                    console.error("ERROR:", errorMsg);
                    setText(`Error: ${errorMsg}`);
                    return reject(new Error(errorMsg));
                }

                navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user', width: { ideal: 640 }, height: { ideal: 480 } } }) // Request specific size if possible
                    .then(stream => {
                        console.log("Webcam stream obtained.");
                        webcamElement.srcObject = stream;
                        webcamElement.addEventListener("loadeddata", () => {
                            console.log("Webcam metadata loaded.");
                            setText("Webcam Ready.");
                            resolve(webcamElement); // Resolve with the video element
                        }, { once: true }); // Ensure listener runs only once
                         webcamElement.addEventListener("error", (err) => {
                            console.error("Webcam element error:", err);
                            setText("Error loading webcam video.");
                            reject(new Error("Webcam element error"));
                         });
                    })
                    .catch(error => {
                         console.error("ERROR: getUserMedia failed:", error.name, error.message, error);
                         setText(`Webcam Error: ${error.name}`);
                         reject(error); // Pass the original error
                    });
            });
        }

        function loadModelGLTF(file) {
             console.log(`Attempting to load 3D model: ${file}`);
             setText("Loading 3D Model...");
             return new Promise((resolve, reject) => {
                const loader = new THREE.GLTFLoader();
                loader.load(file,
                    function (gltf) { // onSuccess
                        console.log("3D Model loaded successfully.");
                        setText("3D Model Ready.");
                        resolve(gltf.scene);
                    },
                    function (xhr) { // onProgress (optional)
                        // console.log(`GLTF Loading Progress: ${(xhr.loaded / xhr.total * 100).toFixed(0)}% loaded`);
                         if (xhr.lengthComputable) {
                             const percentComplete = (xhr.loaded / xhr.total * 100).toFixed(0);
                             setText(`Loading 3D Model: ${percentComplete}%`);
                         } else {
                             setText(`Loading 3D Model: ${xhr.loaded} bytes`);
                         }
                    },
                    function (error) { // onError
                        console.error(`ERROR: Failed to load 3D model (${file}):`, error);
                        setText("Error loading 3D model.");
                        reject(error); // Pass the original error
                    }
                );
            });
        }

        async function trackFace() {
            // Ensure everything is initialized before starting
            if (!model || !glasses || !webcamElement || webcamElement.readyState < webcamElement.HAVE_METADATA || !renderer || !scene || !camera || !outputCtx) {
                 console.warn("trackFace called before initialization complete. Waiting...");
                 rafId = requestAnimationFrame(trackFace); // Try again next frame
                 return;
            }

            const video = webcamElement;

            // Estimate faces
            let faces = [];
            try {
                 faces = await model.estimateFaces({
                    input: video,
                    returnTensors: false,
                    flipHorizontal: false,
                    predictIrises: false
                });
            } catch (error) {
                console.error("Error during face estimation:", error);
                setText("Face estimation error.");
                // Potentially stop the loop or try to recover
                rafId = requestAnimationFrame(trackFace); // Keep trying?
                return;
            }


             // Clear the 2D canvas (video background) and draw the current video frame (flipped)
             outputCtx.clearRect(0, 0, outputCanvas.width, outputCanvas.height); // Important for transparency if needed later
             outputCtx.drawImage(
                video,
                0, 0, video.videoWidth, video.videoHeight,
                0, 0, outputCanvas.width, outputCanvas.height
             );

            // Clear the 3D overlay canvas
            renderer.clear(); // Clears color, depth, stencil

            if (faces.length > 0) {
                // --- Use only the first detected face ---
                const face = faces[0];

                // Optional: Draw bounding box for debugging
                // const { topLeft, bottomRight } = face.boundingBox;
                // drawLine(outputCtx, topLeft[0], topLeft[1], bottomRight[0], topLeft[1]);
                // drawLine(outputCtx, bottomRight[0], topLeft[1], bottomRight[0], bottomRight[1]);
                // drawLine(outputCtx, bottomRight[0], bottomRight[1], topLeft[0], bottomRight[1]);
                // drawLine(outputCtx, topLeft[0], bottomRight[1], topLeft[0], topLeft[1]);

                // --- Glasses Positioning and Scaling ---
                 try {
                    const midwayEyes = face.annotations.midwayBetweenEyes[0];
                    const noseBottom = face.annotations.noseBottom[0];
                    const leftEyeUpper = face.annotations.leftEyeUpper1[3];
                    const rightEyeUpper = face.annotations.rightEyeUpper1[3];

                    // Position based on eyes midpoint
                    glasses.position.x = midwayEyes[0];
                    glasses.position.y = -midwayEyes[1] + yOffsetFactor; // Invert Y + apply offset
                    glasses.position.z = -camera.position.z + midwayEyes[2]; // Approximate Z

                    // Calculate eye distance for base scale
                    const eyeDist = Math.sqrt(
                       (leftEyeUpper[0] - rightEyeUpper[0]) ** 2 +
                       (leftEyeUpper[1] - rightEyeUpper[1]) ** 2 +
                       (leftEyeUpper[2] - rightEyeUpper[2]) ** 2
                    );

                    // Apply Scaling (Adjust base divisor 6.5 if needed for your model)
                    const baseScale = eyeDist / 6.5;
                    glasses.scale.x = baseScale * widthScaleFactor;
                    glasses.scale.y = baseScale * heightScaleFactor;
                    glasses.scale.z = baseScale * ((widthScaleFactor + heightScaleFactor) / 2); // Simple average scaling for Z

                    // Orientation (Up Vector calculation might need adjustment based on model)
                    const upVec = new THREE.Vector3(
                        midwayEyes[0] - noseBottom[0],
                        -(midwayEyes[1] - noseBottom[1]), // Invert Y
                        midwayEyes[2] - noseBottom[2]
                    ).normalize();

                    // Simplified rotation: Primarily align with camera, adjust roll based on up vector's xy projection
                    glasses.rotation.set(0, Math.PI, 0); // Reset, Y=PI assumes model faces -Z initially
                    glasses.rotation.z = Math.atan2(upVec.y, upVec.x) - Math.PI / 2; // Roll adjustment

                     glasses.visible = true;
                 } catch (err) {
                     console.error("Error processing face annotations:", err, face.annotations);
                     glasses.visible = false; // Hide if error occurs
                 }

            } else {
                 glasses.visible = false; // Hide glasses if no face is detected
            }

            // Render the 3D scene (glasses) onto the overlay canvas
            if (renderer && scene && camera) {
                 renderer.render(scene, camera);
            }

            rafId = requestAnimationFrame(trackFace); // Loop
        }

        // --- Initialization Function ---
        async function initialize() {
            // Stop any previous loop
            if (rafId) {
                cancelAnimationFrame(rafId);
            }

            try {
                console.log("--- Starting Initialization ---");

                // 1. Setup Webcam
                setText("Setting up Webcam...");
                console.log("STEP 1: Calling setupWebcam()");
                const videoElement = await setupWebcam();
                console.log("STEP 1: setupWebcam() successful.");

                 // 2. Wait for video to be playable and get dimensions
                 setText("Waiting for Video Dimensions...");
                 console.log("STEP 2: Waiting for video 'canplay' event...");
                 await new Promise((resolve, reject) => {
                     // Timeout if video doesn't load
                     const timeout = setTimeout(() => {
                         reject(new Error("Video load timeout"));
                     }, 10000); // 10 seconds timeout

                     videoElement.oncanplay = () => {
                         clearTimeout(timeout);
                         console.log("STEP 2: Video 'canplay' event fired.");
                         videoElement.play().then(resolve).catch(reject); // Play video and resolve promise
                     };
                 });
                 console.log("STEP 2: Video is playable.");


                 const videoWidth = videoElement.videoWidth;
                 const videoHeight = videoElement.videoHeight;
                 console.log(`STEP 2: Video dimensions: ${videoWidth}x${videoHeight}`);

                 if (!videoWidth || !videoHeight) {
                    throw new Error("Video dimensions are invalid (0x0). Check webcam feed.");
                 }

                // 3. Setup Canvases
                setText("Setting up Canvases...");
                console.log("STEP 3: Configuring canvas sizes...");
                canvasParent.style.width = `${videoWidth}px`;
                canvasParent.style.height = `${videoHeight}px`;
                outputCanvas.width = videoWidth;
                outputCanvas.height = videoHeight;
                overlayCanvas.width = videoWidth;
                overlayCanvas.height = videoHeight;
                console.log("STEP 3: Canvas sizes set.");

                // Setup 2D context for background video
                outputCtx = outputCanvas.getContext("2d");
                outputCtx.translate(videoWidth, 0); // Flip horizontally for mirror effect
                outputCtx.scale(-1, 1);
                // outputCtx.strokeStyle = "#00FF00"; // Optional: Bounding box color
                // outputCtx.lineWidth = 2;
                console.log("STEP 3: 2D context configured.");


                 // 4. Load Face Detection Model
                 setText("Loading Face Model...");
                 console.log("STEP 4: Loading face landmarks model...");
                 model = await faceLandmarksDetection.load(
                     faceLandmarksDetection.SupportedPackages.mediapipeFacemesh,
                     { maxFaces: 1 } // Optimize for single face
                 );
                 console.log("STEP 4: Face landmarks model loaded.");


                // 5. Setup 3D Scene (Renderer, Camera, Scene, Lights)
                setText("Setting up 3D Scene...");
                console.log("STEP 5: Setting up Three.js Renderer...");
                renderer = new THREE.WebGLRenderer({
                    canvas: overlayCanvas,
                    alpha: true // Transparent background
                });
                renderer.setPixelRatio(window.devicePixelRatio);
                renderer.setSize(videoWidth, videoHeight);
                renderer.outputEncoding = THREE.sRGBEncoding; // Correct color space for GLTF
                renderer.setClearColor(0x000000, 0); // Fully transparent clear color
                console.log("STEP 5: Renderer configured.");

                console.log("STEP 5: Setting up Three.js Camera...");
                const fov = 45;
                const aspect = videoWidth / videoHeight;
                const near = 1; // Adjusted near plane
                const far = 2000;
                camera = new THREE.PerspectiveCamera(fov, aspect, near, far);
                camera.position.x = videoWidth / 2;
                camera.position.y = -videoHeight / 2; // THREE.js Y is up, Canvas Y is down
                camera.position.z = (videoHeight / 2) / Math.tan(THREE.MathUtils.degToRad(fov / 2));
                camera.lookAt(new THREE.Vector3(videoWidth / 2, -videoHeight / 2, 0));
                console.log("STEP 5: Camera configured at Z =", camera.position.z);

                console.log("STEP 5: Setting up Three.js Scene and Lights...");
                scene = new THREE.Scene();
                const ambientLight = new THREE.AmbientLight(0xcccccc, 0.6);
                scene.add(ambientLight);
                const pointLight = new THREE.PointLight(0xffffff, 0.8);
                camera.add(pointLight); // Attach light to camera
                scene.add(camera);
                console.log("STEP 5: Scene and Lights configured.");

                // 6. Load 3D Glasses Model
                console.log("STEP 6: Calling loadModelGLTF()...");
                // *** IMPORTANT: Make sure this path is correct! ***
                glasses = await loadModelGLTF("heart_glasses.gltf");
                // If model is in a subfolder: glasses = await loadModelGLTF("models/heart_glasses.gltf");
                console.log("STEP 6: loadModelGLTF() successful.");

                glasses.visible = false; // Hide until face is detected
                scene.add(glasses);
                console.log("STEP 6: Glasses added to scene.");

                 // 7. Setup UI Event Listeners
                 console.log("STEP 7: Setting up UI listeners...");
                 widthScaleSlider.addEventListener('input', (event) => {
                    widthScaleFactor = parseFloat(event.target.value);
                    widthValueSpan.textContent = widthScaleFactor.toFixed(2);
                 });
                 heightScaleSlider.addEventListener('input', (event) => {
                    heightScaleFactor = parseFloat(event.target.value);
                    heightValueSpan.textContent = heightScaleFactor.toFixed(2);
                 });
                 yOffsetSlider.addEventListener('input', (event) => {
                    yOffsetFactor = parseFloat(event.target.value);
                    yOffsetValueSpan.textContent = yOffsetFactor.toFixed(0);
                 });
                 // Initialize display values
                 widthValueSpan.textContent = widthScaleFactor.toFixed(2);
                 heightValueSpan.textContent = heightScaleFactor.toFixed(2);
                 yOffsetValueSpan.textContent = yOffsetFactor.toFixed(0);
                 console.log("STEP 7: UI listeners configured.");

                // 8. Start Tracking Loop
                setText("Ready!");
                console.log("--- Initialization Complete ---");
                console.log("STEP 8: Starting tracking loop (requestAnimationFrame)...");
                trackFace(); // Start the main loop

             } catch (error) {
                 console.error("--- INITIALIZATION FAILED ---", error);
                 // Display specific error if possible, otherwise generic message
                 const errorMsg = error.message || "Unknown error during initialization.";
                 setText(`Initialization failed: ${errorMsg}`);
                 // Optionally stop webcam stream if it started
                 if (webcamElement && webcamElement.srcObject) {
                    webcamElement.srcObject.getTracks().forEach(track => track.stop());
                    webcamElement.srcObject = null;
                 }
            }
        }

        // --- Run Initialization ---
        initialize(); // Start the whole process

    </script>
</body>
</html>